{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import get_dataset_dir\n",
    "\n",
    "german_df = pd.read_csv(get_dataset_dir(\"financial_phrasebank_german\"))                       \n",
    "french_df = pd.read_csv(get_dataset_dir(\"financial_phrasebank_french\"))\n",
    "spanish_df = pd.read_csv(get_dataset_dir(\"financial_phrasebank_spanish\"))\n",
    "english_df = pd.read_csv(get_dataset_dir(\"financial_sentiment_analysis\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_df = german_df.dropna()\n",
    "french_df = french_df.dropna()\n",
    "spanish_df = spanish_df.dropna()\n",
    "english_df = english_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_df[\"lang\"] = \"de\"\n",
    "french_df[\"lang\"] = \"fr\"\n",
    "spanish_df[\"lang\"] = \"es\"\n",
    "english_df[\"lang\"] = \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df = english_df.rename(columns={\"Sentence\": \"sentence\", \"Sentiment\": \"sentiment\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = pd.concat([german_df, french_df, spanish_df, english_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.to_csv(get_dataset_dir(\"financial_phrasebank_multilingual\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Language Distribution\n",
    "\n",
    "Before creating train and test splits, we need to ensure that data is balanced across all four languages (German, French, Spanish, and English) in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language distribution in the dataset:\n",
      "lang\n",
      "de    5842\n",
      "fr    5842\n",
      "es    5842\n",
      "en    5842\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total samples: 23368\n",
      "\n",
      "Sentiment distribution by language:\n",
      "sentiment  negative  neutral  positive\n",
      "lang                                  \n",
      "de              860     3130      1852\n",
      "en              860     3130      1852\n",
      "es              860     3130      1852\n",
      "fr              860     3130      1852\n"
     ]
    }
   ],
   "source": [
    "# Check language distribution in the original dataset\n",
    "print(\"Language distribution in the dataset:\")\n",
    "lang_distribution = final_dataset['lang'].value_counts()\n",
    "print(lang_distribution)\n",
    "print(f\"\\nTotal samples: {len(final_dataset)}\")\n",
    "\n",
    "# Check sentiment distribution by language\n",
    "print(\"\\nSentiment distribution by language:\")\n",
    "sentiment_by_lang = final_dataset.groupby(['lang', 'sentiment']).size().unstack()\n",
    "print(sentiment_by_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Stratified Split by Language\n",
    "\n",
    "To ensure balanced representation across languages in both train and test sets, we'll use a stratified split based on the 'lang' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (16357, 3)\n",
      "Test set shape: (7011, 3)\n",
      "\n",
      "Language distribution in train set:\n",
      "lang\n",
      "en    4090\n",
      "fr    4089\n",
      "es    4089\n",
      "de    4089\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Language distribution in test set:\n",
      "lang\n",
      "de    1753\n",
      "fr    1753\n",
      "es    1753\n",
      "en    1752\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Replace the previous simple split with a stratified split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert the dataframe to a format suitable for Hugging Face datasets\n",
    "train_df, test_df = train_test_split(\n",
    "    final_dataset,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=final_dataset['lang']  # Stratify by language\n",
    ")\n",
    "\n",
    "print(\"Train set shape:\", train_df.shape)\n",
    "print(\"Test set shape:\", test_df.shape)\n",
    "\n",
    "# Verify language distribution in train and test sets\n",
    "print(\"\\nLanguage distribution in train set:\")\n",
    "print(train_df['lang'].value_counts())\n",
    "print(\"\\nLanguage distribution in test set:\")\n",
    "print(test_df['lang'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment distribution in train set:\n",
      "sentiment  negative  neutral  positive\n",
      "lang                                  \n",
      "de              591     2225      1273\n",
      "en              617     2205      1268\n",
      "es              605     2236      1248\n",
      "fr              571     2160      1358\n",
      "\n",
      "Sentiment distribution in test set:\n",
      "sentiment  negative  neutral  positive\n",
      "lang                                  \n",
      "de              269      905       579\n",
      "en              243      925       584\n",
      "es              255      894       604\n",
      "fr              289      970       494\n"
     ]
    }
   ],
   "source": [
    "# Check sentiment distribution in train and test sets\n",
    "print(\"Sentiment distribution in train set:\")\n",
    "print(train_df.groupby(['lang', 'sentiment']).size().unstack())\n",
    "print(\"\\nSentiment distribution in test set:\")\n",
    "print(test_df.groupby(['lang', 'sentiment']).size().unstack())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Balancing (if needed)\n",
    "\n",
    "If the languages have very different sample sizes, we might want to balance them by either:\n",
    "1. Undersampling the majority languages\n",
    "2. Oversampling the minority languages\n",
    "\n",
    "Let's implement a function to balance the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_languages(df, max_samples_per_lang=None):\n",
    "    \"\"\"\n",
    "    Balance the dataset by ensuring equal representation of each language.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the multilingual data\n",
    "        max_samples_per_lang: Maximum samples per language. If None, uses the minimum count across languages.\n",
    "    \n",
    "    Returns:\n",
    "        Balanced DataFrame\n",
    "    \"\"\"\n",
    "    # Get language counts\n",
    "    lang_counts = df['lang'].value_counts()\n",
    "    \n",
    "    # Determine sample size per language\n",
    "    if max_samples_per_lang is None:\n",
    "        max_samples_per_lang = lang_counts.min()\n",
    "    \n",
    "    # Sample equal amounts from each language\n",
    "    balanced_dfs = []\n",
    "    for lang in lang_counts.index:\n",
    "        lang_df = df[df['lang'] == lang]\n",
    "        # Ensure we also maintain sentiment distribution within each language\n",
    "        stratified_sample = lang_df.groupby('sentiment', group_keys=False)\n",
    "        stratified_sample = stratified_sample.apply(lambda x: x.sample(\n",
    "            n=min(len(x), int(max_samples_per_lang * len(x) / len(lang_df))),\n",
    "            random_state=42\n",
    "        ))\n",
    "        balanced_dfs.append(stratified_sample)\n",
    "    \n",
    "    # Combine all balanced language dataframes\n",
    "    balanced_df = pd.concat(balanced_dfs, ignore_index=True)\n",
    "    \n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before balancing - Train language counts: {'en': 4090, 'fr': 4089, 'es': 4089, 'de': 4089}\n",
      "Before balancing - Test language counts: {'de': 1753, 'fr': 1753, 'es': 1753, 'en': 1752}\n",
      "\n",
      "After balancing - Train language counts: {'en': 4090, 'fr': 4089, 'es': 4089, 'de': 4089}\n",
      "After balancing - Test language counts: {'de': 1753, 'fr': 1753, 'es': 1753, 'en': 1752}\n"
     ]
    }
   ],
   "source": [
    "# Apply balancing to train and test sets (if needed)\n",
    "# Uncomment these lines if you want to enforce complete balance\n",
    "\n",
    "# Check if balancing is needed\n",
    "train_lang_counts = train_df['lang'].value_counts()\n",
    "test_lang_counts = test_df['lang'].value_counts()\n",
    "\n",
    "print(f\"Before balancing - Train language counts: {train_lang_counts.to_dict()}\")\n",
    "print(f\"Before balancing - Test language counts: {test_lang_counts.to_dict()}\")\n",
    "\n",
    "# Apply balancing if there's significant imbalance\n",
    "if train_lang_counts.max() / train_lang_counts.min() > 1.5:  # Threshold for imbalance\n",
    "    print(\"Balancing train dataset...\")\n",
    "    train_df = balance_languages(train_df)\n",
    "    \n",
    "if test_lang_counts.max() / test_lang_counts.min() > 1.5:  # Threshold for imbalance\n",
    "    print(\"Balancing test dataset...\")\n",
    "    test_df = balance_languages(test_df)\n",
    "    \n",
    "print(f\"\\nAfter balancing - Train language counts: {train_df['lang'].value_counts().to_dict()}\")\n",
    "print(f\"After balancing - Test language counts: {test_df['lang'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentiment', 'sentence', 'lang', '__index_level_0__'],\n",
      "        num_rows: 16357\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentiment', 'sentence', 'lang', '__index_level_0__'],\n",
      "        num_rows: 7011\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas dataframes to Hugging Face datasets\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Create DatasetDict\n",
    "dataset_dict = datasets.DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV files for later use\n",
    "train_df.to_csv(\"../data/train_subset.csv\", index=False)\n",
    "test_df.to_csv(\"../data/eval_subset.csv\", index=False)\n",
    "\n",
    "# Also save to parquet format (more efficient for Hugging Face datasets)\n",
    "final_dataset.to_parquet(get_dataset_dir(\"financial_phrasebank_multilingual.parquet\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b77d618f0614a5bb68cdf742763125b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891db702ddfb49928b97107472a19afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/17 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8bc90a36b1a4d388f786b45e00a9853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0561a773cca45f894fc80a2bc514bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/nojedag/financial_phrasebank_multilingual/commit/e426e31523a84d438fa2981000c61ef4079192cf', commit_message='Upload dataset', commit_description='', oid='e426e31523a84d438fa2981000c61ef4079192cf', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/nojedag/financial_phrasebank_multilingual', endpoint='https://huggingface.co', repo_type='dataset', repo_id='nojedag/financial_phrasebank_multilingual'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the balanced dataset to Hugging Face Hub\n",
    "dataset_dict.push_to_hub(\"nojedag/financial_phrasebank_multilingual\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
