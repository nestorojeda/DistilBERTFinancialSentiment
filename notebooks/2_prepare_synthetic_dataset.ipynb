{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "740343fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the path so Python can find the toolbox package\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    print(f\"Added {module_path} to sys.path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a18c1fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from toolbox.utils import get_dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80a65cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataste_dir = get_dataset_dir(\"synthetic_financial_sentiment/partial\", ext=None, only_dir=True)\n",
    "\n",
    "# List all files in the dataset directory\n",
    "files = os.listdir(dataste_dir)\n",
    "final_dataset = pd.DataFrame()\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for file in files:\n",
    "    df = pd.read_csv(f'{dataste_dir}/{file}') \n",
    "    # Append the DataFrame to the final dataset\n",
    "    final_dataset = pd.concat([final_dataset, df], ignore_index=True)\n",
    "\n",
    "label2id = {\"neutral\": 0, \"positive\": 1, \"negative\": 2}\n",
    "final_dataset['sentiment'] = final_dataset['sentiment'].map(label2id)\n",
    "final_dataset['sentiment'] = final_dataset['sentiment'].astype(int)\n",
    "final_dataset = final_dataset[['sentence', 'sentiment', 'lang']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3d150b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language distribution in the dataset:\n",
      "lang\n",
      "es    97\n",
      "fr    97\n",
      "de    93\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total samples: 287\n",
      "\n",
      "Sentiment distribution by language:\n",
      "sentiment   0   1   2\n",
      "lang                 \n",
      "de         18  36  39\n",
      "es         18  37  42\n",
      "fr         18  37  42\n"
     ]
    }
   ],
   "source": [
    "# Check language distribution in the original dataset\n",
    "print(\"Language distribution in the dataset:\")\n",
    "lang_distribution = final_dataset['lang'].value_counts()\n",
    "print(lang_distribution)\n",
    "print(f\"\\nTotal samples: {len(final_dataset)}\")\n",
    "\n",
    "# Check sentiment distribution by language\n",
    "print(\"\\nSentiment distribution by language:\")\n",
    "sentiment_by_lang = final_dataset.groupby(['lang', 'sentiment']).size().unstack()\n",
    "print(sentiment_by_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f26a2288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (200, 3)\n",
      "Test set shape: (87, 3)\n",
      "\n",
      "Language distribution in train set:\n",
      "lang\n",
      "fr    68\n",
      "es    67\n",
      "de    65\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Language distribution in test set:\n",
      "lang\n",
      "es    30\n",
      "fr    29\n",
      "de    28\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Replace the previous simple split with a stratified split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert the dataframe to a format suitable for Hugging Face datasets\n",
    "train_df, test_df = train_test_split(\n",
    "    final_dataset,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=final_dataset['lang']  # Stratify by language\n",
    ")\n",
    "\n",
    "print(\"Train set shape:\", train_df.shape)\n",
    "print(\"Test set shape:\", test_df.shape)\n",
    "\n",
    "# Verify language distribution in train and test sets\n",
    "print(\"\\nLanguage distribution in train set:\")\n",
    "print(train_df['lang'].value_counts())\n",
    "print(\"\\nLanguage distribution in test set:\")\n",
    "print(test_df['lang'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e8f9c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment distribution in train set:\n",
      "sentiment   0   1   2\n",
      "lang                 \n",
      "de         11  26  28\n",
      "es         14  26  27\n",
      "fr         11  28  29\n",
      "\n",
      "Sentiment distribution in test set:\n",
      "sentiment  0   1   2\n",
      "lang                \n",
      "de         7  10  11\n",
      "es         4  11  15\n",
      "fr         7   9  13\n"
     ]
    }
   ],
   "source": [
    "# Check sentiment distribution in train and test sets\n",
    "print(\"Sentiment distribution in train set:\")\n",
    "print(train_df.groupby(['lang', 'sentiment']).size().unstack())\n",
    "print(\"\\nSentiment distribution in test set:\")\n",
    "print(test_df.groupby(['lang', 'sentiment']).size().unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbaef50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2118f8a23931481e9402f22fa9b4ec02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89776b0af1694f2e96456df38da5ccb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/87 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc98e66e84bf41319e906f59bae96146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/87 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4f0e61746243929805818d5da7b8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'sentiment', 'lang'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'sentiment', 'lang'],\n",
      "        num_rows: 87\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas dataframes to Hugging Face datasets\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "test_dataset = Dataset.from_pandas(test_df, preserve_index=False)\n",
    "\n",
    "sentiments = [\"neutral\", \"positive\", \"negative\"]\n",
    "langs = [\"en\", \"fr\", \"de\", \"es\"]\n",
    "\n",
    "train_dataset = train_dataset.cast_column(\"lang\", datasets.ClassLabel(names=langs))\n",
    "test_dataset = test_dataset.cast_column(\"lang\", datasets.ClassLabel(names=langs))\n",
    "test_dataset = test_dataset.cast_column(\"sentiment\", datasets.ClassLabel(names=sentiments))\n",
    "train_dataset = train_dataset.cast_column(\"sentiment\", datasets.ClassLabel(names=sentiments))\n",
    "\n",
    "\n",
    "# Create DatasetDict\n",
    "dataset_dict = datasets.DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34e16b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as CSV files for later use\n",
    "train_df.to_csv(\"../data/train_subset.csv\", index=False)\n",
    "test_df.to_csv(\"../data/eval_subset.csv\", index=False)\n",
    "\n",
    "# Also save to parquet format (more efficient for Hugging Face datasets)\n",
    "final_dataset.to_parquet(\n",
    "    get_dataset_dir(\"synthetic_financial_sentiment/synthetic_financial_sentiment_multilingual\", \"parquet\"),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "final_dataset.to_csv(\n",
    "    get_dataset_dir(\"synthetic_financial_sentiment/synthetic_financial_sentiment_multilingual\", \"csv\"),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "974262cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe07fbb172a5408f967b7e4b4e4fafed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f87debf64eaf4136ba03aac13d80d1d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e34d99231b4e9a9ec1c3d1b7222739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3ec549258a41068b8cfad76a1ada03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9169cba7a64347bb08d4a6bd01c827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/796 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/nojedag/synthetic_financial_sentiment/commit/5e3c0dae9c83a31bf5857aabc2e4a588b65e8da1', commit_message='Upload dataset', commit_description='', oid='5e3c0dae9c83a31bf5857aabc2e4a588b65e8da1', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/nojedag/synthetic_financial_sentiment', endpoint='https://huggingface.co', repo_type='dataset', repo_id='nojedag/synthetic_financial_sentiment'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict.push_to_hub(\"nojedag/synthetic_financial_sentiment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
