{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2e8311f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added e:\\repo\\DistilBERTFinancialSentiment to sys.path\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the path so Python can find the toolbox package\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    print(f\"Added {module_path} to sys.path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c349080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "ds = load_dataset(\"TimKoornstra/financial-tweets-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e9f12eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df = ds[\"train\"].to_pandas()\n",
    "ds_df[\"sentence\"] = ds_df[\"tweet\"].astype(str)\n",
    "ds_df.drop(columns=[\"tweet\", \"url\"], inplace=True)\n",
    "ds_df[\"lang\"] = \"en\"\n",
    "\n",
    "# Remove urls from the text\n",
    "def remove_urls(text):\n",
    "    return ' '.join(word for word in text.split() if not word.startswith('http'))\n",
    "\n",
    "# remove retweets\n",
    "def remove_retweets(text):\n",
    "    return text.split(\"RT @\")[0].strip()\n",
    "\n",
    "# remove strings larger than 512 characters\n",
    "def remove_large_strings(text):\n",
    "    return text if len(text) <= 512 else text[:512]\n",
    "\n",
    "ds_df[\"sentence\"] = ds_df[\"sentence\"].apply(remove_urls)\n",
    "ds_df[\"sentence\"] = ds_df[\"sentence\"].apply(remove_retweets)\n",
    "ds_df[\"sentence\"] = ds_df[\"sentence\"].apply(remove_large_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e16ae7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe_en_fr = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\", device=0, batch_size=32, truncation=True)\n",
    "pipe_en_de = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-de\", device=0, batch_size=32, truncation=True)\n",
    "pipe_en_es = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\", device=0, batch_size=32, truncation=True)\n",
    "\n",
    "pipe_collection = {\n",
    "    \"fr\": pipe_en_fr,\n",
    "    \"de\": pipe_en_de,\n",
    "    \"es\": pipe_en_es,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3b7348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, tokenizer, max_tokens=512):\n",
    "    tokens = tokenizer.encode(text, truncation=False)\n",
    "    chunks = [tokens[i:i+max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
    "    return [tokenizer.decode(chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "915ac66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = ds_df.copy()\n",
    "\n",
    "for lang, pipe in pipe_collection.items():\n",
    "    lang_df = ds_df.copy()\n",
    "    sentence_list = ds_df[\"sentence\"].tolist()\n",
    "    translated_sentences = pipe(sentence_list)\n",
    "    translated_sentences = [sentence[\"translation_text\"] for sentence in translated_sentences]\n",
    "    lang_df[\"sentence\"] = translated_sentences\n",
    "    lang_df[\"lang\"] = lang\n",
    "    result_df = pd.concat([result_df, lang_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "703a02fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language distribution in the dataset:\n",
      "lang\n",
      "en    38091\n",
      "fr    38091\n",
      "de    38091\n",
      "es    38091\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total samples: 152364\n",
      "\n",
      "Sentiment distribution by language:\n",
      "sentiment      0      1     2\n",
      "lang                         \n",
      "de         12181  17368  8542\n",
      "en         12181  17368  8542\n",
      "es         12181  17368  8542\n",
      "fr         12181  17368  8542\n"
     ]
    }
   ],
   "source": [
    "print(\"Language distribution in the dataset:\")\n",
    "lang_distribution = result_df['lang'].value_counts()\n",
    "print(lang_distribution)\n",
    "print(f\"\\nTotal samples: {len(result_df)}\")\n",
    "\n",
    "# Check sentiment distribution by language\n",
    "print(\"\\nSentiment distribution by language:\")\n",
    "sentiment_by_lang = result_df.groupby(['lang', 'sentiment']).size().unstack()\n",
    "print(sentiment_by_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96d54288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (106654, 3)\n",
      "Test set shape: (45710, 3)\n",
      "\n",
      "Language distribution in train set:\n",
      "lang\n",
      "en    26664\n",
      "fr    26664\n",
      "de    26663\n",
      "es    26663\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Language distribution in test set:\n",
      "lang\n",
      "de    11428\n",
      "es    11428\n",
      "en    11427\n",
      "fr    11427\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Replace the previous simple split with a stratified split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert the dataframe to a format suitable for Hugging Face datasets\n",
    "train_df, test_df = train_test_split(\n",
    "    result_df,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=result_df['lang']  # Stratify by language\n",
    ")\n",
    "\n",
    "print(\"Train set shape:\", train_df.shape)\n",
    "print(\"Test set shape:\", test_df.shape)\n",
    "\n",
    "# Verify language distribution in train and test sets\n",
    "print(\"\\nLanguage distribution in train set:\")\n",
    "print(train_df['lang'].value_counts())\n",
    "print(\"\\nLanguage distribution in test set:\")\n",
    "print(test_df['lang'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4c972cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment distribution in train set:\n",
      "sentiment     0      1     2\n",
      "lang                        \n",
      "de         8472  12181  6010\n",
      "en         8573  12068  6023\n",
      "es         8543  12139  5981\n",
      "fr         8486  12192  5986\n",
      "\n",
      "Sentiment distribution in test set:\n",
      "sentiment     0     1     2\n",
      "lang                       \n",
      "de         3709  5187  2532\n",
      "en         3608  5300  2519\n",
      "es         3638  5229  2561\n",
      "fr         3695  5176  2556\n"
     ]
    }
   ],
   "source": [
    "# Check sentiment distribution in train and test sets\n",
    "print(\"Sentiment distribution in train set:\")\n",
    "print(train_df.groupby(['lang', 'sentiment']).size().unstack())\n",
    "print(\"\\nSentiment distribution in test set:\")\n",
    "print(test_df.groupby(['lang', 'sentiment']).size().unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bae083c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f6ba6c3d674bd2aa028110430dada6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/106654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad17bcd968144cdc8dc862dd63ac2a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/45710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8459a526282b4340938572882b9b13a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/45710 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2975e5cd71d453da113e4625ee126e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/106654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentiment', 'sentence', 'lang'],\n",
      "        num_rows: 106654\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentiment', 'sentence', 'lang'],\n",
      "        num_rows: 45710\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_df, preserve_index=False)\n",
    "\n",
    "sentiments = [\"neutral\", \"positive\", \"negative\"]\n",
    "langs = [\"en\", \"fr\", \"de\", \"es\"]\n",
    "\n",
    "train_dataset = train_dataset.cast_column(\"lang\", datasets.ClassLabel(names=langs))\n",
    "test_dataset = test_dataset.cast_column(\"lang\", datasets.ClassLabel(names=langs))\n",
    "test_dataset = test_dataset.cast_column(\"sentiment\", datasets.ClassLabel(names=sentiments))\n",
    "train_dataset = train_dataset.cast_column(\"sentiment\", datasets.ClassLabel(names=sentiments))\n",
    "\n",
    "# Create DatasetDict\n",
    "dataset_dict = datasets.DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2bdb74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class weights for each sentiment class:\n",
      "Sentiment 0: 1.0424\n",
      "Sentiment 1: 0.7311\n",
      "Sentiment 2: 1.4864\n"
     ]
    }
   ],
   "source": [
    "# Calculate the class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'sentiment' is the column with the labels\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(result_df['sentiment']),\n",
    "    y=result_df['sentiment']\n",
    ")\n",
    "\n",
    "# Convert class weights to a dictionary\n",
    "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "# Print the class weights so they can be copied and pasted into the code\n",
    "# or used directly in the training script\n",
    "print(\"\\nClass weights for each sentiment class:\")\n",
    "for sentiment, weight in class_weights_dict.items():\n",
    "    print(f\"Sentiment {sentiment}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93c4280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox.utils import get_dataset_dir\n",
    "\n",
    "# Save as CSV files for later use\n",
    "train_df.to_csv(\"../data/_new/train_subset.csv\", index=False)\n",
    "test_df.to_csv(\"../data/_new/eval_subset.csv\", index=False)\n",
    "\n",
    "# Also save to parquet format (more efficient for Hugging Face datasets)\n",
    "result_df.to_parquet(\n",
    "    get_dataset_dir(\"financial_phrasebank_multilingual/financial_phrasebank_multilingual\", \"parquet\"),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "result_df.to_csv(\n",
    "    get_dataset_dir(\"financial_phrasebank_multilingual/financial_phrasebank_multilingual\", \"csv\"),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23e0ddf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5290637544c342d5904fa95621a9e876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0c892c2b884b7f9e37382a8ee84ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/107 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a44f12a8ee45988c658a950950e2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6afb246e3de4bbcbfb348da971d4a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/46 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afcbc0b935f74187bf93756989d693ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/641 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/nojedag/financial-tweets-sentiment-multilingual/commit/dc0bb890834565db17c34a9180cbd4ce2d6c0f49', commit_message='Upload dataset', commit_description='', oid='dc0bb890834565db17c34a9180cbd4ce2d6c0f49', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/nojedag/financial-tweets-sentiment-multilingual', endpoint='https://huggingface.co', repo_type='dataset', repo_id='nojedag/financial-tweets-sentiment-multilingual'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict.push_to_hub(\"nojedag/financial-tweets-sentiment-multilingual\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
